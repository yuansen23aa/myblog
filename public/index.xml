<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Sen Yuan (袁森) - Learning to Unlearn</title>
    <link>http://localhost:64253/</link>
    <description>Recent content on Sen Yuan (袁森) - Learning to Unlearn</description>
    <generator>Hugo -- 0.154.3</generator>
    <language>en</language>
    <lastBuildDate>Sun, 11 Jan 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:64253/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Transformer Part I: the details that actually matter</title>
      <link>http://localhost:64253/posts/revisit-transformer/</link>
      <pubDate>Sun, 11 Jan 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:64253/posts/revisit-transformer/</guid>
      <description>&lt;p&gt;Transformer is arguably the most influential AI innovation in the past decade, serving as the foundation as our modern day LLM models. It also revolutionize some adjacent fields such as computer vision, recommender system etc. In this note, we are going to revisit this revolutionary technology with a focus on discussing a set of details that matter.&lt;/p&gt;
&lt;h1 id=&#34;transformer-recap&#34;&gt;Transformer Recap&lt;/h1&gt;
&lt;p&gt;Transformer was first proposed as a encoder-decoder model to solve the machine translation problem. Take English to Chinese language translation as our example, encoder is basically encoding text tokens in a way such that both individual token meaning and joint dependency can be captured. The output is the tensor representation of English sentences. The decoder is responsible for text generation and learning relationship between English and Chines. The text generation process is basically next token prediction which is achieved by masking unseen tokens so attention is only paid to preceding tokens in the sentence, namely causal masking. The English and Chineses relationship is learned by cross attention where query is Chinese tokens, which pays more attention to tokens from encoded English output with high attention weights.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transformer Part I: the details that actually matter</title>
      <link>http://localhost:64253/posts/transformer_part1/</link>
      <pubDate>Sun, 11 Jan 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:64253/posts/transformer_part1/</guid>
      <description>&lt;p&gt;Transformer is arguably the most influential AI innovation in the past decade, serving as the foundation as our modern day LLM models. It also revolutionize some adjacent fields such as computer vision, recommender system etc. In this note, we are going to revisit this revolutionary technology with a focus on discussing a set of details that matter.&lt;/p&gt;
&lt;h1 id=&#34;transformer-recap&#34;&gt;Transformer Recap&lt;/h1&gt;
&lt;p&gt;Transformer was first proposed as a encoder-decoder model to solve the machine translation problem. Take English to Chinese language translation as our example, encoder is basically encoding text tokens in a way such that both individual token meaning and joint dependency can be captured. The output is the tensor representation of English sentences. The decoder is responsible for text generation and learning relationship between English and Chines. The text generation process is basically next token prediction which is achieved by masking unseen tokens so attention is only paid to preceding tokens in the sentence, namely causal masking. The English and Chineses relationship is learned by cross attention where query is Chinese tokens, which pays more attention to tokens from encoded English output with high attention weights.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why I Started Writing</title>
      <link>http://localhost:64253/posts/my-first-post/</link>
      <pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:64253/posts/my-first-post/</guid>
      <description>&lt;p&gt;2026 is shaping up to be another exciting year for AI and technology. One of my New Year’s resolutions is to start a personal website to document my learning journey and the “aha” moments I encounter along the way, both in technology and in life. So why writing, and why now?&lt;/p&gt;
&lt;p&gt;Some might argue that large language models already explain technical concepts so well that writing blog posts no longer adds much value. I see it differently.Here are my reasons.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduction to Contrastive Loss</title>
      <link>http://localhost:64253/posts/introduction-to-contrastive-loss/</link>
      <pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:64253/posts/introduction-to-contrastive-loss/</guid>
      <description>&lt;p&gt;Contrastive Loss is a widely used objective in &lt;strong&gt;metric learning&lt;/strong&gt; and &lt;strong&gt;contrastive learning&lt;/strong&gt;.&lt;br&gt;
Its goal is to learn an embedding space where &lt;strong&gt;similar samples are close together&lt;/strong&gt;, while &lt;strong&gt;dissimilar samples are far apart&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The loss operates on &lt;strong&gt;pairs of samples&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Positive pairs&lt;/strong&gt;: two samples that should be considered similar&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Negative pairs&lt;/strong&gt;: two samples that should be considered different&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given a pair of embeddings and a binary label, contrastive loss:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;penalizes &lt;strong&gt;large distances&lt;/strong&gt; between positive pairs&lt;/li&gt;
&lt;li&gt;penalizes &lt;strong&gt;small distances&lt;/strong&gt; between negative pairs (up to a margin)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This encourages the model to learn representations that are discriminative and geometry-aware.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
