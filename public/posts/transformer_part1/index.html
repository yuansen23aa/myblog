<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Transformer Part I: the details that actually matter | Sen Yuan (袁森) - Learning to Unlearn</title>
<meta name="keywords" content="">
<meta name="description" content="Transformer is arguably the most influential AI innovation in the past decade, serving as the foundation as our modern day LLM models. It also revolutionize some adjacent fields such as computer vision, recommender system etc. In this note, we are going to revisit this revolutionary technology with a focus on discussing a set of details that matter.
Transformer Recap
Transformer was first proposed as a encoder-decoder model to solve the machine translation problem. Take English to Chinese language translation as our example, encoder is basically encoding text tokens in a way such that both individual token meaning and joint dependency can be captured. The output is the tensor representation of English sentences. The decoder is responsible for text generation and learning relationship between English and Chines. The text generation process is basically next token prediction which is achieved by masking unseen tokens so attention is only paid to preceding tokens in the sentence, namely causal masking. The English and Chineses relationship is learned by cross attention where query is Chinese tokens, which pays more attention to tokens from encoded English output with high attention weights.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/transformer_part1/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css" integrity="sha256-2jIR5e&#43;Ge/K3X9WmUVz&#43;1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/transformer_part1/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
    <script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Sen Yuan (袁森) - Learning to Unlearn (Alt + H)">Sen Yuan (袁森) - Learning to Unlearn</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search">
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Transformer Part I: the details that actually matter
    </h1>
    <div class="post-meta"><span title='2026-01-11 00:00:00 +0000 UTC'>January 11, 2026</span>&nbsp;·&nbsp;<span>11 min</span>

</div>
  </header> 
  <div class="post-content"><p>Transformer is arguably the most influential AI innovation in the past decade, serving as the foundation as our modern day LLM models. It also revolutionize some adjacent fields such as computer vision, recommender system etc. In this note, we are going to revisit this revolutionary technology with a focus on discussing a set of details that matter.</p>
<h1 id="transformer-recap">Transformer Recap<a hidden class="anchor" aria-hidden="true" href="#transformer-recap">#</a></h1>
<p>Transformer was first proposed as a encoder-decoder model to solve the machine translation problem. Take English to Chinese language translation as our example, encoder is basically encoding text tokens in a way such that both individual token meaning and joint dependency can be captured. The output is the tensor representation of English sentences. The decoder is responsible for text generation and learning relationship between English and Chines. The text generation process is basically next token prediction which is achieved by masking unseen tokens so attention is only paid to preceding tokens in the sentence, namely causal masking. The English and Chineses relationship is learned by cross attention where query is Chinese tokens, which pays more attention to tokens from encoded English output with high attention weights.</p>
<p>In this note, we are only interested the decoder part as the decoder-only model is actually being adopted by most succesful LLMs such as GPT. Most building blocks and technical details are similar between encoder and decoder.</p>
<h1 id="the-decoder-only-transformer-overview">The decoder-only transformer overview<a hidden class="anchor" aria-hidden="true" href="#the-decoder-only-transformer-overview">#</a></h1>
<p>Let&rsquo;s think about what are some most important elements going from tokens to next token generation.</p>
<ol>
<li>Token and its embedding: we need tokenization to covert sentences into list of index (the maximum index is the token vocab size), and we need embedding lookup table to lookup embedding for each index.</li>
<li>Token and their relative position: we need to consider the relative position of those tokens as putting words together in different order have different meanings.</li>
<li>Depedency learning: we need to know the joint relationship between tokens, that is, for the current token, which tokens in the sentence looks more relevant.</li>
<li>Diversified dependencies: depencies have different patterns: subject -&gt; verb: the dog runs, verb -&gt; direct:  object: i buy a car, phonatactic pattern etc.</li>
</ol>
<p>For each of the above 4 elements, transformer has corresponding supporting components. From input to output:</p>
<ol>
<li>sequence of tokens and their embeddings: for training, we chop text corpus into sequences, and let&rsquo;s take asequence of tokens as our input unit. The sequence of tokens will generate an embedding matrix by looking up token embedding table.</li>
<li>position encoding: position encoding will encode the relative postion from 0 to L-1 where L is the context window/sentence length. The resulting position embedding will be added to sequence token embeddings which will serve as the input for self attention module next.</li>
<li>masked self attention: the input for self attention is still individual token sequence without talking to each other. The most important role of self attention is to force the current token talks to earlier tokens in the context and transform current token to make it context-aware. This is the heart of the transformer and we will delve into it later.</li>
<li>multi-head attention: having only one attention head can only learn one type of dependency, that&rsquo;s why it&rsquo;s dessirable to have multiple heads for learning different dependent patterns.</li>
</ol>
<p>In addition to the above components, the transformer also implemented add &amp; norm where add is using x + f(x) type of residual connection to enhance optiomization for deep networks, particularly addressing gradient vanishing problem. The norm here refers to layer norm which normalize token-wise embeddings, we will explain later why this matters. The multi-head attention blocks are stacked sequentially to increase the depth of the network. Overall design is like below.</p>
<p><img alt="Transformer attention" loading="lazy" src="/posts/transformer_part1/transformer.png"></p>
<h1 id="technical-deep-dive">Technical deep dive<a hidden class="anchor" aria-hidden="true" href="#technical-deep-dive">#</a></h1>
<p>Now we dive into some key technical aspects of transformers to understand the intuition behind them.</p>
<table>
  <thead>
      <tr>
          <th>Notation</th>
          <th>Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>d</td>
          <td>The dimension of token emb &amp; positional emb</td>
      </tr>
      <tr>
          <td>L</td>
          <td>The length of token sequence</td>
      </tr>
      <tr>
          <td>B</td>
          <td>Batch size</td>
      </tr>
      <tr>
          <td>H</td>
          <td>Head size</td>
      </tr>
      <tr>
          <td>V</td>
          <td>Vocab size</td>
      </tr>
      <tr>
          <td>N</td>
          <td>Number of Multi-head attention blocks</td>
      </tr>
      <tr>
          <td>x</td>
          <td>The input for attention block, shape (L, d)</td>
      </tr>
      <tr>
          <td>$W^q, W^k, W^v$</td>
          <td>projection matrix to project input x to q, k, v, shape (d, d/H)</td>
      </tr>
      <tr>
          <td>q, k, v</td>
          <td>vector representation for token level attention, shape (d/H,1)</td>
      </tr>
      <tr>
          <td>Q, K, V</td>
          <td>matrix representation for all tokens in sequnece for attention, shape (L, d/H), e.g., $Q = [q_1^T,&hellip;,q_L^T]^T$</td>
      </tr>
  </tbody>
</table>
<h2 id="self-attention">Self Attention<a hidden class="anchor" aria-hidden="true" href="#self-attention">#</a></h2>
<p>I remember the first time i read the &ldquo;Attention is all you need&rdquo; paper years back, i was really puzzled about what Q,K,V really means. It is easier if we first start pondering what&rsquo;s needed in the output. So, the input now is embedding vectors for each individual tokens plus some positional embeddings, which is still token-wise information without communicating with each other. What&rsquo;s really desired is to let token to communicate and transform itself in such a way that more relevant tokens will play a bigger role in the final output. Mathematically, we need to consider current_token_emb_output = sum over all tokens { similarity(current_token, token) * token }. So the target we are transformation is our Query, the token for which is used for similarity calculation is Key, and the token value that&rsquo;s being averaged over is Value. The naming makes a lot of sense because Query is trying to search for keys that are most relevant and the corresponding values are convolutionized by the affinity between query and key.</p>
<p>At the individual token level, $ output = \sum_{i=1}^L sim(q, k_i) v_i $ where $L$ is the length of the sequence and $q, k_i, v_i$ are $d$ dimensional vectors. A natural way to calculate sim is dot product $ q^T k_i = \sum_{j=1}^d q_{j} k_{ji}$, assuming each element of $q$ and $k$ are mutually i.i.d Normal(0,1), the variance of the dot product is $d$, to make the dot product standardized normal, we need to rescale it therefore $sim = q^Tk_i/\sqrt{d}$. However, the $sim$ function is only standardized across $d$ token embedding entries, not really normalized across $L$ tokens, namely, we need to rescale $sim(q, k_i)$ such that $\sum_{i=1}^L sim(q, k_i) = 1$. The rescaling is important to bound the variance the final output. A typical way to standardize $sim$ is to use softmax, which is differentiable compared to absolute values. Now, let&rsquo;s switch to matrix notation for compactness, $Q = [q_1^T, &hellip;, q_L^T]^T$, similar for $K, V$. The final output $(L, d)$ matrix is
$output = Softmax(QK^T/\sqrt{d})V$.</p>
<p>Maybe one detail worth mentioning here is going from the input embedding matrix $x$ to $q, k, v$, we add learnable projection matrix $W^q, W^k, W^v$ to make $q, k ,v$ data driven and learnable, namely, $q, k, v = xW^q, xW^k, xW^v$. The dimension for matrix $W$ usually is set to $(d, d/H)$ where $H$ is the number of attention heads. The reason is for consistency because after self attetion transformation from individual head, we&rsquo;ll concatenate output from each head and $d/H * H = d$ will make the output has the same dimension as token embedding dimension.</p>
<h2 id="masking">Masking<a hidden class="anchor" aria-hidden="true" href="#masking">#</a></h2>
<p>Let&rsquo;s say if the sentence &ldquo;The biggest animal on earth is the blue whale, with a heart the size of a small car&rdquo; is in our training data, now if someone ask transformer &ldquo;the biggest animal on earth is&rdquo;, we need to sample from the tokens and aim to ensure $P(the \quad blue \quad whale| the \quad biggest \quad animal \quad on \quad earth \quad is)$ has an extremely high probablity. So you see, the setenece after &ldquo;the blue whale&rdquo; like &ldquo;with a heart&hellip;&rdquo; does not really matter during token generation. Therefore, if our goal is to generate token with maximum likelihood given preceding tokens in the sentence, we have to ensure the current token only pay attention to earlier tokens. This requires revising the attention calculation.</p>
<p>Let&rsquo;s say our query $q_t$ is at postion $t$ in the sequence, then $q_t = \sum_{i=1}^t sim(q_t, k_i) v_i$ not sum over all L tokens. The matrix form is shown in the picture above with the lower triangular matrix form.</p>
<p>To make sure the the model generation and target are aligned, we also need to shift the input by 1 position, say if the target sentence is Target = &ldquo;The biggest animal on earth is the blue whale&rdquo;, Input =  &ldquo;&lt;\START&gt; The biggest animal on earth is the blue&rdquo; if our token is just word. Without this, we would expect the output from &ldquo;the&rdquo; is &ldquo;biggest&rdquo;, but target is still &ldquo;the&rdquo; which will lead to unexpected loss.</p>
<h2 id="add--norm">Add &amp; Norm<a hidden class="anchor" aria-hidden="true" href="#add--norm">#</a></h2>
<p>The Add &amp; Norm refers to residual connection and layer norm. Let&rsquo;s first look at the residual connection: $x + f(x)$, the idea is simple, for the transformation, the output is not $f(x)$, instead its $y = x + f(x)$. This equivalently is using $f(x) = y - x$ to learn the residual between output and input. The residual connection is applied to both attention head and FFN. Why this is important ? Let&rsquo;s consider a deep neural networks $y = f_1 (f_2 ..,(f_M (x,w)))$, then $\frac{dy}{dw} = \partial f_1 * {&hellip;}  *\partial f_{M}$, and if lots of the partial derivatives are between 0 and 1, the gradient of w will vanish as a result, the deeper the network, the more likely it will happen. Now if we switch to residual connection $y = f_1 (. + f_2(&hellip;, x + f_M(x, w)))$,
$\frac{dy}{dw} = (1 + \partial f_1) * {&hellip;}  * (1+\partial f_{M})$ which won&rsquo;t suffer from gradient vanishing thanks to adding the input back to the transformation.</p>
<p>For the layer norm, besides generally stablizing the network and reducing the training time. One important reason why it&rsquo;s so crucial for transformation is because it forces the normalization chain is consistently maintained, as aformentioned we apply $1/\sqrt{d}$ scaling factor to dot product to standardize variance, which is based on the <em><strong>ASSUMPTION</strong></em> that each entry in $q$ and $k$ vector are normalized to standard normal distribution. The make the asusmption hold, we need to apply layer norm to first standardize the embedding vector for each token in the sequence so that $x = (x_1, &hellip;, x_L)$ matrix where $x_i$ is d dimensional, $x_{ij} = \frac{x_{ij} - mean(x_i)}{std(x_i)}$, here we ignored the tunable parameters in layernorm for illustration purpose. Then $q, k , v = x W^q, x W^k, x W^v $ are standardized as a result.</p>
<p>Actually, there&rsquo;s one more normalization needs to be implemented, in a more implicit way. Remember, inside the attention block,
we have two residual connection steps: y = x + MHA(x); then z = y + FFN(y); which will cause variance inflation as we increase number of blocks/layers. Therefore it&rsquo;s desirable to rescale $z = \frac{z}{\sqrt{2N}}$ in the weight initialization for residual branch.</p>
<p><img alt="Normalization Flow" loading="lazy" src="/posts/transformer_part1/norm.png"></p>
<h2 id="model-complexity">Model Complexity<a hidden class="anchor" aria-hidden="true" href="#model-complexity">#</a></h2>
<p>Let&rsquo;s go over the parameter size and algorithm complexity for transformer.
We consider a single sequence in the batch for the estimation.</p>
<p>The estimation should be straightforward. For attention blocks, we have three major steps. For convenience, we ignore multi-head setting and use d as the dimension for both input token emb size and projection matrix output dim.</p>
<p>Step 1. Projection:  For projection from input x to Q, K, V, the flop is also $O(L d^2)$. The projection result is $(L,d)$ and each entry is generated by dot product of $d$ dimensional vector.</p>
<p>Step 2. Attention score: The $L^2 d$ is for  attention mechanism because the output is $L \times L$ and each output entry requires a  $d$ dimensional dot product compututation, which leads to $L^2d$.</p>
<p>Step 3. For FFN, the output is $L \times d$
dimensional and each entry is the result from a $d$ dimensional dot product, which leads to $Ld^2$.</p>
<table>
  <thead>
      <tr>
          <th>Component</th>
          <th>Parameter Size</th>
          <th>Complexity/FLOP</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Token emb Lookup</td>
          <td>$V \times d $</td>
          <td>$O(L)$</td>
      </tr>
      <tr>
          <td>Positional emb lookup</td>
          <td>$L \times d$</td>
          <td>$O(L)$</td>
      </tr>
      <tr>
          <td>Layernorm</td>
          <td>$2d$</td>
          <td>$O(Ld)$</td>
      </tr>
      <tr>
          <td>Multi-head attention blocks</td>
          <td>$const \times d^2 \times N$</td>
          <td>$O(L^2 d + L d^2)$</td>
      </tr>
      <tr>
          <td>LM head</td>
          <td>$Vd$</td>
          <td>$ O(VdL)$</td>
      </tr>
  </tbody>
</table>
<h3 id="practical-implications">Practical Implications<a hidden class="anchor" aria-hidden="true" href="#practical-implications">#</a></h3>
<p>Large vocabulary size (V), long context window (L), deeper networks ((N)), and larger model dimension (d) all increase model cost, but in different ways:</p>
<ul>
<li>(L) primarily affects <strong>FLOPs and memory</strong></li>
<li>(d) and (N) primarily affect <strong>parameter count</strong></li>
</ul>
<p>This explains why models such as <strong>GPT-3</strong> scale mainly through large (d) and (N), while avoiding absolute positional embeddings for long-context settings.</p>
<h1 id="closing-thoughts--next-episode">Closing Thoughts &amp; Next Episode<a hidden class="anchor" aria-hidden="true" href="#closing-thoughts--next-episode">#</a></h1>
<p>In this note, we go through the algorithmic details of the decoder-only transformer, aka, causal transformer.
The heart of the transformer is the attention block where two sub components are stacked: masked self attention and feed forward network. The attention&rsquo;s job is communication and convolution, transforming each token&rsquo;s embedding into contexualized token embedding. The feed forward network is processing information at token level, reorganizing data coming from different attention heads (concatenated output) into a better representation of the token.</p>
<p>I personally view self attention as a mathematically projection, if we ignore the softmax normalization and parameterization of k and v, $output = \sum_{i=1}^T &lt;q, x_i&gt; x_i $ is basically projection of q onto the subsapce spanned by $(x_1, &hellip; ,x_n)$, which seems really plausible. In simple language, what attention is doing is to find the best combination of seen words embeddings to re-represent the query embedding.</p>
<p>In the next episode, we will dig into the implementation details and some fun toy experiments to gain deeper insights about this beast.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformer Part I: the details that actually matter on x"
            href="https://x.com/intent/tweet/?text=Transformer%20Part%20I%3a%20the%20details%20that%20actually%20matter&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer_part1%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformer Part I: the details that actually matter on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer_part1%2f&amp;title=Transformer%20Part%20I%3a%20the%20details%20that%20actually%20matter&amp;summary=Transformer%20Part%20I%3a%20the%20details%20that%20actually%20matter&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer_part1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformer Part I: the details that actually matter on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer_part1%2f&title=Transformer%20Part%20I%3a%20the%20details%20that%20actually%20matter">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformer Part I: the details that actually matter on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer_part1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformer Part I: the details that actually matter on whatsapp"
            href="https://api.whatsapp.com/send?text=Transformer%20Part%20I%3a%20the%20details%20that%20actually%20matter%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer_part1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformer Part I: the details that actually matter on telegram"
            href="https://telegram.me/share/url?text=Transformer%20Part%20I%3a%20the%20details%20that%20actually%20matter&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer_part1%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformer Part I: the details that actually matter on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Transformer%20Part%20I%3a%20the%20details%20that%20actually%20matter&u=http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer_part1%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">Sen Yuan (袁森) - Learning to Unlearn</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
