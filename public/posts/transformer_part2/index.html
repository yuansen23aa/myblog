<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Transformer Part II: The implementation &amp; experiments | Sen Yuan (袁森) - Learning to Unlearn</title>
<meta name="keywords" content="">
<meta name="description" content="In this episode, we will go into some details of the causal transfomer implementation. Some toy experiment results are shown to analyze transformer, in an attempt to understand what drives the performance. We will not go through every single line of implementation. The code used for illustration can be found: https://github.com/yuansen23aa/GPT-learning/blob/main/basic_gpt.ipynb, which largely follows Anrej Karpathy&rsquo;s nanoGPT implementation with some modifications. So Let&rsquo;s dig in.
We use those terms interchangeably: block size = sequence length, causal attention = masked attention, causal transformation = decoder-only transformer.">
<meta name="author" content="">
<link rel="canonical" href="https://www.yuansen.xyz/posts/transformer_part2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css" integrity="sha256-2jIR5e&#43;Ge/K3X9WmUVz&#43;1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://www.yuansen.xyz/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.yuansen.xyz/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.yuansen.xyz/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://www.yuansen.xyz/apple-touch-icon.png">
<link rel="mask-icon" href="https://www.yuansen.xyz/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://www.yuansen.xyz/posts/transformer_part2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="https://www.yuansen.xyz/posts/transformer_part2/">
  <meta property="og:site_name" content="Sen Yuan (袁森) - Learning to Unlearn">
  <meta property="og:title" content="Transformer Part II: The implementation & experiments">
  <meta property="og:description" content="In this episode, we will go into some details of the causal transfomer implementation. Some toy experiment results are shown to analyze transformer, in an attempt to understand what drives the performance. We will not go through every single line of implementation. The code used for illustration can be found: https://github.com/yuansen23aa/GPT-learning/blob/main/basic_gpt.ipynb, which largely follows Anrej Karpathy’s nanoGPT implementation with some modifications. So Let’s dig in.
We use those terms interchangeably: block size = sequence length, causal attention = masked attention, causal transformation = decoder-only transformer.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2026-01-15T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-01-15T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Transformer Part II: The implementation &amp; experiments">
<meta name="twitter:description" content="In this episode, we will go into some details of the causal transfomer implementation. Some toy experiment results are shown to analyze transformer, in an attempt to understand what drives the performance. We will not go through every single line of implementation. The code used for illustration can be found: https://github.com/yuansen23aa/GPT-learning/blob/main/basic_gpt.ipynb, which largely follows Anrej Karpathy&rsquo;s nanoGPT implementation with some modifications. So Let&rsquo;s dig in.
We use those terms interchangeably: block size = sequence length, causal attention = masked attention, causal transformation = decoder-only transformer.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://www.yuansen.xyz/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Transformer Part II: The implementation \u0026 experiments",
      "item": "https://www.yuansen.xyz/posts/transformer_part2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformer Part II: The implementation \u0026 experiments",
  "name": "Transformer Part II: The implementation \u0026 experiments",
  "description": "In this episode, we will go into some details of the causal transfomer implementation. Some toy experiment results are shown to analyze transformer, in an attempt to understand what drives the performance. We will not go through every single line of implementation. The code used for illustration can be found: https://github.com/yuansen23aa/GPT-learning/blob/main/basic_gpt.ipynb, which largely follows Anrej Karpathy\u0026rsquo;s nanoGPT implementation with some modifications. So Let\u0026rsquo;s dig in.\nWe use those terms interchangeably: block size = sequence length, causal attention = masked attention, causal transformation = decoder-only transformer.\n",
  "keywords": [
    
  ],
  "articleBody": "In this episode, we will go into some details of the causal transfomer implementation. Some toy experiment results are shown to analyze transformer, in an attempt to understand what drives the performance. We will not go through every single line of implementation. The code used for illustration can be found: https://github.com/yuansen23aa/GPT-learning/blob/main/basic_gpt.ipynb, which largely follows Anrej Karpathy’s nanoGPT implementation with some modifications. So Let’s dig in.\nWe use those terms interchangeably: block size = sequence length, causal attention = masked attention, causal transformation = decoder-only transformer.\nOverall structure Let’s peel the onion and see layer by layer of the overall structure.\nGPT = Embbeding Lookup -\u003e Attention Blocks -\u003e LM\nAttention Blocks = [Multihead Self Attention -\u003e Feed Forward Network]\nMultihead Self Attention = Concat All Single Masked Self Attention Head\nFollowing this structure, we need to implement the following classes: GPT, Attention Blocks, Multihead Attention, Masked Self Attention, Feed Forward Network.\nGPT class:\nForward: map X id matrix (batch size, block size) into token embedding tensor (batch size, block size, emb size), create position embedding (block size, emb size) and broadcast it to (batch size, block size, emb size) and add position and token embedding together as the input for attention blocks. The attention blocks spit out (batch size, block size, emb size) tensor as LM head input, LM will map emb size to vocab size as logits vector for all batch size * block size tokens. Generation: autoregressive token generation based on input context. Attention blocks: since we first layernorm input x for multihead attention, then run residual connection x + mha(x). Similarly, we do layernorm for FFN and residual connection sequetially.\nMultihead attention: the main role is concatenation\nMasked self attention: implement Q, K, V and masked attention head.\nFeed Forward Network: projecting multihead attention output to 4*emb size hidden layer with GeLu, then projecting to the output with size emb size.\nBug prone components Instead of checking the entire implementation, we highlight the parts that are essential but error prone.\nShift Input Position by 1 class dataloaderlite: def __init__(self, data, block_size, batch_size, device, shuffle=True, tag=None): ... ... def __next__(self): ... x = torch.stack([self.data[i:i+self.L] for i in idx]) y = torch.stack([self.data[i+1:i+self.L+1] for i in idx]) return x.to(self.device), y.to(self.device) The last three lines of the dataloader we implemented are crucial because it defines y as 1 poisition ahead of x so that the model is properly defined. Additionally, it registers the input and target to the device (cpu or cuda) used.\nPay attention to the pytorch status/mode @torch.no_grad() def loss_estimation(model, grad_norm=False): output = {} model.eval() .... model.train() return output Whe decorator @torch.no_grad() disables gradient calculation via Not creating computation graph to speed up the inference. When eval started, we enter eval mode so that functionalities such as dropout can be called correctly. Finally, remember to switch back to train mode when eval is done.\nInput reshaping for cross entropy loss class GPT(nn.Module): ... def forward(self, x_id_matrix, y_id_matrix=None): ... else: targets = y_id_matrix.view(Batch_size * Block_size) logits = logits.view(Batch_size * Block_size, Vocab_size) The reshaping flatterns both targets and logits so inputs can be accepted by the cross entropy loss. Bascially, you can think of Batches of Blocks as a single long block by stitching batches one after another. Given the memory is laid out by row-first internally, the view() will treat the last dimension of logits unchanged (vocab size) and will collaps the first two dimensions.\nMasked Attention class MaskedAttention(nn.Module): def __init__(self, num_heads, embed_size): super().__init__() self.head_size = embed_size // num_heads self.q_proj = nn.Linear(embed_size, self.head_size, bias=False) self.k_proj = nn.Linear(embed_size, self.head_size, bias=False) self.v_proj = nn.Linear(embed_size, self.head_size, bias=False) def forward(self, x): B, L, _ = x.shape q, k ,v = self.q_proj(x), self.k_proj(x), self.v_proj(x) kt = k.transpose(-1,-2) att = q @ kt/ (self.head_size**0.5) mask = torch.tril(torch.ones(B, L, L, device=device, requires_grad=False)) att = att.masked_fill(mask == 0, float('-inf')) weights = F.softmax(att, dim=-1) weights = self.dropout(weights) att_output = weights @ v return att_output It is important to note that we implement maksing through masking elements above the diagonal in the attention weight matrix. Feel free to compare this implementation to the transformer part I to map the math to the implementation here. Last but not least, remember to register the device to the mask trigular matrix!\nQuestion: why we get B, L from input x instead of using global batch size and block size ? Will reveal the answer in a moment.\nAutoregressive Generation def generate(self, x_id_matrix, max_new_tokens, top_k): B, L = x_id_matrix.shape out = x_id_matrix.clone() for _ in range(max_new_tokens): if x_id_matrix.shape[1] \u003e self.block_size: x_id_matrix = x_id_matrix[:, -self.block_size:] logits, _ = self(x_id_matrix) # focus only on the last time step logits = logits[:, -1, :] # (B, vocab_size) top_k_logits, top_k_indices = torch.topk(logits, k=top_k) top_k_probs = F.softmax(top_k_logits, dim=-1) # (B, vocab_size) # sample from the distribution sampled = torch.multinomial(top_k_probs, num_samples=1) # (B, 1) next_id = torch.gather(top_k_indices, -1, sampled) # (B, 1) # append sampled index to the running sequence x_id_matrix= torch.cat((x_id_matrix, next_id), dim=1) # (B, L+1) out = torch.cat((out, next_id), dim=1) return out The token generation is worth a closer look because the autoregressive generation is quite deferent from inference during training. Here x_id_matrix is our prompt, we can start with any arbitrary prompt with length no greater than global block size. Say if we start with torch.zeros((1,1)), it basically says batch size = 1 and the first token index is 0. The autoregressive token generation will sample the next token according to the predicted probabilties and add it to the current x_id_matrix and repeat the process.\ntruncation: since the x_id_matrix will keep growing and once it hits the context window length limit: block_size, we have to truncate it and only keep the last block_size token for token generation: $P(t_{k} | t_{k-1}, …, t_{t - L})$ where $L$ is block size.\nattention inference: it’s time to reveal the answer to the question why using local B, L = x.shape. The reason is because during text generation, B and L are decided by conext or x_id_matrix. In the very beginning, L is just 1 and the number will dynamically change as we keep adding generated token index to x_id_matrix.\ntop k rule: top k rule is defined to eliminate noisy candidates with small probablities so we will only sample token from the top k tokens ranked by logits.\nExperiments We will identify drivers of the good performance of causal transformer. We use the same dataset (tiny shakespare) used by https://www.youtube.com/watch?v=kCc8FmEb1nY, it has 40,000 lines of Shakespare from a varienty of Shakespare’s plays. The dataset is tiny so our goal is to see how much validation loss we can achieve and how close the generated text is to English sentences.\nAs a baseline comparison, we implemented a basic FFN and the best validation loss is around 2.500, even after we increase the model size, the loss stays roughly the same. The generated text looks better than random, but apparently departs nontrivially from English.\nI'd payou ead me BAThoweve, Tof s, qurarore gerilyrsondghys R blere ar urss bld u d wachaigureap anuneilf an ppirear a g th nof nd wicowhy: Byowindiveatirw'su quenglle V: wha pe ghat wsod fut'd f aphay: Lowilichesor us s CO t:--lerand oof giearelos s'd alal me was Thome n forind Talingnen at n ubu sse pr s whee tate's st fumy t s, se wahfongisow, ARD: Coomo, geind, Fire ARCImowir ARAng; Yond aleed y, ARDo wnd by beake, THangaithilond y, thenct t he, wize isard oures: We will first start with basic transformer with following setup\nbatch_size = 32 # how many independent sequences will we process in parallel? block_size = 8 # what is the maximum context length for predictions? max_iters = 4000 # the number of training iterations learning_rate = 1e-3 # learning rate embed_size = 64 # embedding size num_heads = 4 num_blocks = 2 The loss will go sharply down to 2.038 vs. 2.500 (baseline) and generated text below looks closer to English and the structure looks closer to Shakespears’ plays. Residual Connection is essential, without which the performance will crash, the loss will grow to 3.34 and generate nonsense. In this regard, Attention is not all we need, we also need residual connection.\nANTH: SwO, but madions, treing,' and And hand youful death: That sing and's hor butse hath he me; as the hearth A const acto-se of the murdindifords the mads ack ying a sair oness, As still hearld with in in, Had comition Thork what morrus to that wearnief station ands to at house. LARDII: Had it. LUCIO: Sayonsher, Bution thy the what sits there in say Scaling Law \u0026 Weight Init Although we train on a tiny dataset, increasing model capacity seems to work better on reducing validation loss. First we increase the block size by 4x to increase the model size through position emb scaling, the loss reduction is clear. Second, we 2x the depth by double the number of blocks, the validation loss can be reduced to 1.631 vs. 2.038 (no scaling). We also started to see many English words to appear!\nIt is worth pointing out that apply weight init with std = 0.02 further improves the performace with loss = 1.610. He be sun whister it be sway To his atch'd your limiss bound Of him over sompetie, and flight, from the publiness Bestista'd and to friend of this senden for you. KING RICHARD II: But it bate, and warred to my chare-lip thy life. CAPULET: Now, such I cress the cause of the pause of warrior And can speak toge in hear as towe they broak! AGOM: I my doubt in thy lady poor, and, If their safeth; whose while have abjoy. But wwhat acquital me that I have heard the poor. Why citizen him sin, and thr Summary We covered the implementation and experiment results to demonstrate the sueprior performance of decoder only transformer. In the next episode, we will visit the topic of efficiency including KV cache, FlashAttention, PageAttention and so on.\n",
  "wordCount" : "1649",
  "inLanguage": "en",
  "datePublished": "2026-01-15T00:00:00Z",
  "dateModified": "2026-01-15T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.yuansen.xyz/posts/transformer_part2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sen Yuan (袁森) - Learning to Unlearn",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.yuansen.xyz/favicon.ico"
    }
  }
}
</script>
    <script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.yuansen.xyz/" accesskey="h" title="Sen Yuan (袁森) - Learning to Unlearn (Alt + H)">Sen Yuan (袁森) - Learning to Unlearn</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.yuansen.xyz/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://www.yuansen.xyz/search/" title="Search">
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://www.yuansen.xyz/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Transformer Part II: The implementation &amp; experiments
    </h1>
    <div class="post-meta"><span title='2026-01-15 00:00:00 +0000 UTC'>January 15, 2026</span>&nbsp;·&nbsp;<span>8 min</span>

</div>
  </header> 
  <div class="post-content"><p>In this episode, we will go into some details of the causal transfomer implementation. Some toy experiment results are shown to analyze transformer, in an attempt to understand what drives the performance. We will not go through every single line of implementation. The code used for illustration can be found: <a href="https://github.com/yuansen23aa/GPT-learning/blob/main/basic_gpt.ipynb">https://github.com/yuansen23aa/GPT-learning/blob/main/basic_gpt.ipynb</a>, which largely follows Anrej Karpathy&rsquo;s nanoGPT implementation with some modifications. So Let&rsquo;s dig in.</p>
<p>We use those terms interchangeably: block size = sequence length, causal attention = masked attention, causal transformation = decoder-only transformer.</p>
<h1 id="overall-structure">Overall structure<a hidden class="anchor" aria-hidden="true" href="#overall-structure">#</a></h1>
<p>Let&rsquo;s peel the onion and see layer by layer of the overall structure.</p>
<ul>
<li>
<p>GPT  = Embbeding Lookup -&gt; <strong>Attention Blocks</strong> -&gt; LM</p>
</li>
<li>
<p>Attention Blocks = [<strong>Multihead Self Attention</strong> -&gt; Feed Forward Network]</p>
</li>
<li>
<p>Multihead Self Attention = Concat All Single <strong>Masked Self Attention</strong> Head</p>
</li>
</ul>
<p>Following this structure, we need to implement the following classes: GPT, Attention Blocks, Multihead Attention, Masked Self Attention, Feed Forward Network.</p>
<ul>
<li>
<p><strong>GPT class</strong>:</p>
<ul>
<li><strong>Forward</strong>: map  X id matrix (batch size, block size) into token embedding tensor (batch size, block size, emb size), create position embedding (block size, emb size) and broadcast it to (batch size, block size, emb size) and add position and token embedding together as the input for attention blocks. The attention blocks spit out (batch size, block size, emb size) tensor as LM head input, LM will map emb size to vocab size as logits vector for all batch size * block size tokens.</li>
<li><strong>Generation</strong>: autoregressive token generation based on input context.</li>
</ul>
</li>
<li>
<p><strong>Attention blocks</strong>: since we first layernorm input x for multihead attention, then run residual connection x + mha(x). Similarly, we do layernorm for FFN and residual connection sequetially.</p>
</li>
<li>
<p><strong>Multihead attention</strong>: the main role is concatenation</p>
</li>
<li>
<p><strong>Masked self attention</strong>: implement Q, K, V and masked attention head.</p>
</li>
<li>
<p><strong>Feed Forward Network</strong>: projecting multihead attention output to 4*emb size hidden layer with GeLu, then projecting to the output with size emb size.</p>
</li>
</ul>
<h1 id="bug-prone-components">Bug prone components<a hidden class="anchor" aria-hidden="true" href="#bug-prone-components">#</a></h1>
<p>Instead of checking the entire implementation, we highlight the parts that are essential but error prone.</p>
<h2 id="shift-input-position-by-1">Shift Input Position by 1<a hidden class="anchor" aria-hidden="true" href="#shift-input-position-by-1">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">dataloaderlite</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, data, block_size, batch_size, device, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, tag<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__next__</span>(self):
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">...</span>   
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack([self<span style="color:#f92672">.</span>data[i:i<span style="color:#f92672">+</span>self<span style="color:#f92672">.</span>L] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> idx])
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack([self<span style="color:#f92672">.</span>data[i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>:i<span style="color:#f92672">+</span>self<span style="color:#f92672">.</span>L<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> idx])    
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device), y<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span></code></pre></div><p>The last three lines of the dataloader we implemented are crucial because it defines y as 1 poisition ahead of x so that the model is properly defined. Additionally, it registers the input and target to the device (cpu or cuda) used.</p>
<h2 id="pay-attention-to-the-pytorch-statusmode">Pay attention to the pytorch status/mode<a hidden class="anchor" aria-hidden="true" href="#pay-attention-to-the-pytorch-statusmode">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@torch.no_grad</span>()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_estimation</span>(model, grad_norm<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>    output <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">....</span>
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>train()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> output
</span></span></code></pre></div><p>Whe decorator @torch.no_grad() disables gradient calculation via Not creating computation graph to speed up the inference. When eval started, we enter eval mode so that functionalities such as dropout can be called correctly. Finally, remember to switch back to train mode when eval is done.</p>
<h2 id="input-reshaping-for-cross-entropy-loss">Input reshaping for cross entropy loss<a hidden class="anchor" aria-hidden="true" href="#input-reshaping-for-cross-entropy-loss">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GPT</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x_id_matrix, y_id_matrix<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            targets <span style="color:#f92672">=</span> y_id_matrix<span style="color:#f92672">.</span>view(Batch_size <span style="color:#f92672">*</span> Block_size)
</span></span><span style="display:flex;"><span>            logits <span style="color:#f92672">=</span> logits<span style="color:#f92672">.</span>view(Batch_size <span style="color:#f92672">*</span> Block_size, Vocab_size)
</span></span></code></pre></div><p>The reshaping flatterns both targets and logits so inputs can be accepted by the cross entropy loss. Bascially, you can think of Batches of Blocks as a single long block by stitching batches one after another. Given the memory is laid out by row-first internally, the view() will treat the last dimension of logits unchanged (vocab size) and will collaps the first two dimensions.</p>
<h2 id="masked-attention">Masked Attention<a hidden class="anchor" aria-hidden="true" href="#masked-attention">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MaskedAttention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, num_heads, embed_size):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>head_size <span style="color:#f92672">=</span> embed_size <span style="color:#f92672">//</span> num_heads
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>q_proj <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(embed_size, self<span style="color:#f92672">.</span>head_size, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>k_proj <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(embed_size, self<span style="color:#f92672">.</span>head_size, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>v_proj <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(embed_size, self<span style="color:#f92672">.</span>head_size, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        B, L, _ <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        q, k ,v <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>q_proj(x), self<span style="color:#f92672">.</span>k_proj(x), self<span style="color:#f92672">.</span>v_proj(x)
</span></span><span style="display:flex;"><span>        kt <span style="color:#f92672">=</span> k<span style="color:#f92672">.</span>transpose(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        att <span style="color:#f92672">=</span> q <span style="color:#f92672">@</span> kt<span style="color:#f92672">/</span> (self<span style="color:#f92672">.</span>head_size<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>        mask <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tril(torch<span style="color:#f92672">.</span>ones(B, L, L, device<span style="color:#f92672">=</span>device, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>))
</span></span><span style="display:flex;"><span>        att <span style="color:#f92672">=</span> att<span style="color:#f92672">.</span>masked_fill(mask <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, float(<span style="color:#e6db74">&#39;-inf&#39;</span>))
</span></span><span style="display:flex;"><span>        weights <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(att, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        weights <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(weights)
</span></span><span style="display:flex;"><span>        att_output <span style="color:#f92672">=</span> weights <span style="color:#f92672">@</span> v
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> att_output 
</span></span></code></pre></div><p>It is important to note that we
implement maksing through masking elements above the diagonal in the attention weight matrix. Feel free to compare this implementation to the transformer part I to map the math to the implementation here. Last but not least, remember to register the device to the mask trigular matrix!</p>
<p>Question: why we get B, L from input x instead of using global batch size and block size ?
Will reveal the answer in a moment.</p>
<h2 id="autoregressive-generation">Autoregressive Generation<a hidden class="anchor" aria-hidden="true" href="#autoregressive-generation">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>   <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">generate</span>(self, x_id_matrix, max_new_tokens, top_k):
</span></span><span style="display:flex;"><span>        B, L <span style="color:#f92672">=</span> x_id_matrix<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> x_id_matrix<span style="color:#f92672">.</span>clone()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(max_new_tokens):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> x_id_matrix<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">&gt;</span> self<span style="color:#f92672">.</span>block_size:
</span></span><span style="display:flex;"><span>                x_id_matrix <span style="color:#f92672">=</span> x_id_matrix[:, <span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>block_size:]   
</span></span><span style="display:flex;"><span>            logits, _ <span style="color:#f92672">=</span> self(x_id_matrix)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># focus only on the last time step</span>
</span></span><span style="display:flex;"><span>            logits <span style="color:#f92672">=</span> logits[:, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, :] <span style="color:#75715e"># (B, vocab_size)</span>
</span></span><span style="display:flex;"><span>            top_k_logits, top_k_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>topk(logits, k<span style="color:#f92672">=</span>top_k)
</span></span><span style="display:flex;"><span>            top_k_probs <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(top_k_logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># (B, vocab_size)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># sample from the distribution</span>
</span></span><span style="display:flex;"><span>            sampled <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>multinomial(top_k_probs, num_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># (B, 1)</span>
</span></span><span style="display:flex;"><span>            next_id <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>gather(top_k_indices, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, sampled)  <span style="color:#75715e"># (B, 1)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># append sampled index to the running sequence</span>
</span></span><span style="display:flex;"><span>            x_id_matrix<span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((x_id_matrix, next_id), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># (B, L+1)</span>
</span></span><span style="display:flex;"><span>            out <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((out, next_id), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span></code></pre></div><p>The token generation is worth a closer look because the autoregressive generation is quite deferent from inference during training. Here x_id_matrix is our prompt, we can start with any arbitrary prompt with length no greater than global block size. Say if we start with torch.zeros((1,1)), it basically says batch size = 1 and the first token index is 0. The autoregressive token generation will sample the next token according to the predicted probabilties and add it to the current x_id_matrix and repeat the process.</p>
<ul>
<li>
<p><strong>truncation</strong>: since the x_id_matrix will keep growing and once it hits the context window length limit: block_size, we have to truncate it and only keep the last block_size token for token generation: $P(t_{k} | t_{k-1}, &hellip;, t_{t - L})$ where $L$ is block size.</p>
</li>
<li>
<p><strong>attention inference</strong>: it&rsquo;s time to reveal the answer to the question why using local B, L = x.shape. The reason is because during text generation, B and L are decided by conext or x_id_matrix. In the very beginning, L is just 1 and the number will dynamically change as we keep adding generated token index to x_id_matrix.</p>
</li>
<li>
<p><strong>top k rule</strong>: top k rule is defined to eliminate noisy candidates with small probablities so we will only sample token from the top k tokens ranked by logits.</p>
</li>
</ul>
<h1 id="experiments">Experiments<a hidden class="anchor" aria-hidden="true" href="#experiments">#</a></h1>
<p>We will identify drivers of the good performance of causal transformer. We use the same dataset (tiny shakespare) used by <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">https://www.youtube.com/watch?v=kCc8FmEb1nY</a>, it has 40,000 lines of Shakespare from a varienty of Shakespare&rsquo;s plays. The dataset is tiny so our goal is to see how much validation loss we can achieve and how close the generated text is to English sentences.</p>
<p>As a baseline comparison, we implemented a basic FFN and the best validation loss is around 2.500, even after we increase the model size, the loss stays roughly the same.  The generated text looks better than random, but apparently departs nontrivially from English.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>I<span style="color:#e6db74">&#39;d payou ead me</span>
</span></span><span style="display:flex;"><span>BAThoweve,
</span></span><span style="display:flex;"><span>Tof s, qurarore gerilyrsondghys R blere ar urss bld u d wachaigureap anuneilf an ppirear a g th nof nd wicowhy:
</span></span><span style="display:flex;"><span>Byowindiveatirw<span style="color:#e6db74">&#39;su quenglle V: wha pe ghat wsod fut&#39;</span>d f aphay:
</span></span><span style="display:flex;"><span>Lowilichesor us s CO t:<span style="color:#f92672">--</span>lerand oof giearelos s<span style="color:#e6db74">&#39;d alal me was</span>
</span></span><span style="display:flex;"><span>Thome n forind Talingnen at n ubu sse pr s whee tate<span style="color:#e6db74">&#39;s st fumy t s, se wahfongisow,</span>
</span></span><span style="display:flex;"><span>ARD:
</span></span><span style="display:flex;"><span>Coomo, geind,
</span></span><span style="display:flex;"><span>Fire
</span></span><span style="display:flex;"><span>ARCImowir
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ARAng;
</span></span><span style="display:flex;"><span>Yond aleed y,
</span></span><span style="display:flex;"><span>ARDo wnd by beake,
</span></span><span style="display:flex;"><span>THangaithilond y, thenct t he, wize isard oures:
</span></span></code></pre></div><p>We will first start with basic transformer with following setup</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span> <span style="color:#75715e"># how many independent sequences will we process in parallel?</span>
</span></span><span style="display:flex;"><span>block_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">8</span> <span style="color:#75715e"># what is the maximum context length for predictions?</span>
</span></span><span style="display:flex;"><span>max_iters <span style="color:#f92672">=</span> <span style="color:#ae81ff">4000</span> <span style="color:#75715e"># the number of training iterations</span>
</span></span><span style="display:flex;"><span>learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-3</span> <span style="color:#75715e"># learning rate</span>
</span></span><span style="display:flex;"><span>embed_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span> <span style="color:#75715e"># embedding size</span>
</span></span><span style="display:flex;"><span>num_heads <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>num_blocks <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>
</span></span></code></pre></div><p>The loss will go sharply down to 2.038 vs. 2.500 (baseline) and generated text below looks closer to English and the structure looks closer to Shakespears&rsquo; plays. <strong>Residual Connection</strong> is essential, without which the performance will crash, the loss will grow to 3.34 and generate nonsense. In this regard, Attention is not all we need, we also need residual connection.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ANTH:
</span></span><span style="display:flex;"><span>SwO, but madions, treing,<span style="color:#e6db74">&#39; and</span>
</span></span><span style="display:flex;"><span>And hand youful death:
</span></span><span style="display:flex;"><span>That sing <span style="color:#f92672">and</span><span style="color:#e6db74">&#39;s hor butse hath he me; as the hearth</span>
</span></span><span style="display:flex;"><span>A const acto<span style="color:#f92672">-</span>se of the murdindifords the mads ack ying a sair oness,
</span></span><span style="display:flex;"><span>As still hearld <span style="color:#66d9ef">with</span> <span style="color:#f92672">in</span> <span style="color:#f92672">in</span>,
</span></span><span style="display:flex;"><span>Had comition
</span></span><span style="display:flex;"><span>Thork what morrus to
</span></span><span style="display:flex;"><span>that wearnief station ands to at house<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>LARDII:
</span></span><span style="display:flex;"><span>Had it<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>LUCIO:
</span></span><span style="display:flex;"><span>Sayonsher,
</span></span><span style="display:flex;"><span>Bution thy the what sits there <span style="color:#f92672">in</span> say
</span></span></code></pre></div><h1 id="scaling-law--weight-init">Scaling Law &amp; Weight Init<a hidden class="anchor" aria-hidden="true" href="#scaling-law--weight-init">#</a></h1>
<p>Although we train on a tiny dataset, increasing model capacity seems to work better on reducing validation loss. First we increase the block size by 4x to increase the model size through position emb scaling, the loss reduction is clear. Second, we 2x the depth by double the number of blocks, the validation loss can be reduced to 1.631 vs. 2.038 (no scaling). We also started to see many English words to appear!</p>
<p>It is worth pointing out that apply weight init with std = 0.02 further improves the performace with loss = 1.610.
<img alt="Validation Loss Curve" loading="lazy" src="/posts/transformer_part2/scaling.png"></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>He be sun whister it be sway
</span></span><span style="display:flex;"><span>To his atch<span style="color:#e6db74">&#39;d your limiss bound</span>
</span></span><span style="display:flex;"><span>Of him over sompetie, <span style="color:#f92672">and</span> flight, <span style="color:#f92672">from</span> the publiness
</span></span><span style="display:flex;"><span>Bestista<span style="color:#e6db74">&#39;d and to friend of this senden for you.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>KING RICHARD II:
</span></span><span style="display:flex;"><span>But it bate, <span style="color:#f92672">and</span> warred to my chare<span style="color:#f92672">-</span>lip thy life<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>CAPULET:
</span></span><span style="display:flex;"><span>Now, such I cress the cause of the pause of warrior
</span></span><span style="display:flex;"><span>And can speak toge <span style="color:#f92672">in</span> hear <span style="color:#66d9ef">as</span> towe they broak<span style="color:#960050;background-color:#1e0010">!</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>AGOM:
</span></span><span style="display:flex;"><span>I my
</span></span><span style="display:flex;"><span>doubt <span style="color:#f92672">in</span> thy lady poor, <span style="color:#f92672">and</span>, If their safeth; whose <span style="color:#66d9ef">while</span> have abjoy<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>But wwhat acquital me that I have heard the poor<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>Why citizen him sin, <span style="color:#f92672">and</span> thr
</span></span></code></pre></div><h1 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">#</a></h1>
<p>We covered the implementation and experiment results to demonstrate the sueprior performance of decoder only transformer. In the next episode, we will visit the topic of efficiency including KV cache, FlashAttention, PageAttention and so on.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformer Part II: The implementation & experiments on x"
            href="https://x.com/intent/tweet/?text=Transformer%20Part%20II%3a%20The%20implementation%20%26%20experiments&amp;url=https%3a%2f%2fwww.yuansen.xyz%2fposts%2ftransformer_part2%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformer Part II: The implementation & experiments on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.yuansen.xyz%2fposts%2ftransformer_part2%2f&amp;title=Transformer%20Part%20II%3a%20The%20implementation%20%26%20experiments&amp;summary=Transformer%20Part%20II%3a%20The%20implementation%20%26%20experiments&amp;source=https%3a%2f%2fwww.yuansen.xyz%2fposts%2ftransformer_part2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformer Part II: The implementation & experiments on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fwww.yuansen.xyz%2fposts%2ftransformer_part2%2f&title=Transformer%20Part%20II%3a%20The%20implementation%20%26%20experiments">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformer Part II: The implementation & experiments on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.yuansen.xyz%2fposts%2ftransformer_part2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformer Part II: The implementation & experiments on whatsapp"
            href="https://api.whatsapp.com/send?text=Transformer%20Part%20II%3a%20The%20implementation%20%26%20experiments%20-%20https%3a%2f%2fwww.yuansen.xyz%2fposts%2ftransformer_part2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformer Part II: The implementation & experiments on telegram"
            href="https://telegram.me/share/url?text=Transformer%20Part%20II%3a%20The%20implementation%20%26%20experiments&amp;url=https%3a%2f%2fwww.yuansen.xyz%2fposts%2ftransformer_part2%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformer Part II: The implementation & experiments on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Transformer%20Part%20II%3a%20The%20implementation%20%26%20experiments&u=https%3a%2f%2fwww.yuansen.xyz%2fposts%2ftransformer_part2%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://www.yuansen.xyz/">Sen Yuan (袁森) - Learning to Unlearn</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
